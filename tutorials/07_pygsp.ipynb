{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] tutorial 7: PyGSP \n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "adapted from [NTDS'17](https://nbviewer.jupyter.org/github/mdeff/ntds_2017/blob/outputs/demos/08_pygsp.ipynb)\n",
    "\n",
    "[Eda Bayram](http://lts4.epfl.ch/bayram), [EPFL LTS4](http://lts4.epfl.ch)\n",
    "\n",
    "This is a tutorial on [PyGSP](https://github.com/epfl-lts2/pygsp), a Python package to ease signal processing on graphs.\n",
    "The PyGSP facilitates a wide variety of operations on graphs, like computing their Fourier basis, filtering or interpolating signals, plotting graphs, signals, and filters. The package includes a wide range of graphs and many filter banks. Despite all the pre-defined models, you can easily use a custom graph by defining its adjacency matrix, and a custom filter bank by defining a set of functions in the spectral domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from pygsp import graphs, filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph creation, models, properties, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs are created with the [graphs module](https://pygsp.readthedocs.io/en/stable/reference/graphs.html). It includes a wide range of graphs:\n",
    "* readily built on a manifold such as;  \n",
    "\n",
    "`graphs.grid2d()`, `graphs.Bunny()`, `graphs.Swissroll()` etc.  \n",
    "* models for generating random graphs such as;\n",
    "\n",
    "`graphs.ErdosRenyi()`, `graphs.BarabasiAlbert()`, `graphs.Sensor()`, `graphs.StochasticBlockModel()` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Constructing a feature graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs can also be constructed from a set of points in $\\mathbb{R}^d$.\n",
    "\n",
    "Let $\\mathbf{X}$ be a data matrix $\\mathbf{X} = [\\mathbf{x}_1, \\ldots, \\mathbf{x}_N]^\\intercal \\in \\mathbb{R}^{N \\times d}$, where each $\\mathbf{x} \\in \\mathbb{R}^d$ is a sample for which we have $d$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100\n",
    "dimensionality = 2\n",
    "points = np.random.uniform(size=(n_points, dimensionality))\n",
    "plt.scatter(points[:, 0], points[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Determine the connectivity pattern:**\n",
    "* kNN graph\n",
    "* $\\epsilon$ - radius graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Determine edge weights with a Gaussian Kernel** \n",
    "$$\\mathbf{W}[i,j] = \\exp(-\\frac{||\\mathbf{x}_i - \\mathbf{x}_j||^2_2 }{ \\sigma^2})$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_NN = graphs.NNGraph(points, NNtype='knn', k=2, sigma = sigma**2, rescale=False, symmetrize_type='maximum')\n",
    "print(f'{G_NN.N} nodes, {G_NN.Ne} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_NN = graphs.NNGraph(points, NNtype='radius', epsilon=0.1, sigma = sigma**2, rescale=False, symmetrize_type='maximum')\n",
    "print(f'{G_NN.N} nodes, {G_NN.Ne} edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Graph from adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = sparse.random(100, 100, 0.02)  # Sparse graph.\n",
    "W.setdiag(0)\n",
    "W = W + W.T\n",
    "\n",
    "G = graphs.Graph(W)\n",
    "print('{} nodes, {} edges'.format(G.N, G.Ne))\n",
    "\n",
    "print('Connected: {}'.format(G.is_connected()))\n",
    "print('Directed: {}'.format(G.is_directed()))\n",
    "\n",
    "plt.hist(G.d)\n",
    "plt.title('Degree distribution of my graph');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Graph layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to plot a graph, we need to embed its nodes in a 2D or 3D space. The process to find coordinates for each nodes is called [layout](https://en.wikipedia.org/wiki/Graph_drawing). Most included graph models define these. If that's not the case, or if you create the graph from an adjacency matrix, [`pygsp.graphs.Graph.set_coordinates()`](https://pygsp.readthedocs.io/en/stable/reference/graphs.html#pygsp.graphs.Graph.set_coordinates) can be used to set it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, the nodes had a natural 2D embedding. Hence, it is automatically plotted using the 2D coordinates enclosed in `points` and the selected nearest neighborhood connectivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_NN.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, to plot the latter graph, we need to set a layout. We can visualize the network within different layouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(17, 5))\n",
    "\n",
    "G.set_coordinates('ring2D')\n",
    "G.plot(ax=axes[0])\n",
    "\n",
    "G.set_coordinates('spring')\n",
    "G.plot(ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spectral graph theory: graph spectrum, Laplacian eigenmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a random community graph using the stochastic block model and perform its spectral analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graphs.StochasticBlockModel(N=200, k=3, p=0.1, q=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the sparsity pattern and the connectivity to visualize the communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(17, 6))\n",
    "axes[0].spy(G.W, markersize=0.5)\n",
    "G.set_coordinates(kind='spring')\n",
    "G.plot(ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral analysis of a graph begins with the eigenvalue decompostion of the graph Laplacian. First we need to compute Laplacian matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.compute_laplacian('combinatorial') \n",
    "G.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the normalized Laplacian by calling `G.compute_laplacian('normalized')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Spectrum of a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few eigenvalues. Recall that for sparse matrices, we may use the optimized eigensolvers from the `sparse` module from `scipy` to obtain the first few eigenvalues and eigenvectors, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, U = sparse.linalg.eigsh(G.L, k=20, which='SM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 5))\n",
    "plt.plot(eig_val, '+-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How many connected components exist in your random network according to the graph spectrum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Eigenmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the first 4 eigenvectors as a signal on the graph using `graphs.plot_signal()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(17, 6))\n",
    "G.plot_signal(U[:, 0], ax=axes[0])\n",
    "G.plot_signal(U[:, 1], ax=axes[1])\n",
    "G.plot_signal(U[:, 2], ax=axes[2])\n",
    "G.plot_signal(U[:, 3], ax=axes[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also visualize the datapoints on a lower dimensional embedding given by the eigenmap of the Laplacian. The first eigenvector's information is useless (as long as we use the combinatorial Laplacian anyways) as it is constant. So, we can plot each node $i$ in 2D with coordinates $(u_2(i), u_3(i))$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(U[:, 1], U[:, 2], c=G.info['node_com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Plot first few eigenvectors of a ring graph as a function of its eigenvalues. What do you observe?\n",
    "\n",
    "* Use `G = graphs.Ring(N=50)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exercise: Spectral Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show that, in the last example, performing $k$-means in the $2D$ plane recovers the 3 communities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Signal and Graph Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a random signal on our graph and plot it on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.random.normal(size=G.N)\n",
    "G.plot_signal(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gradient and divergence\n",
    "\n",
    "The gradient $\\nabla_\\mathcal{G} \\ f$ of the signal $f$ on the graph $\\mathcal{G}$ is a signal on the edges defined as\n",
    "\n",
    "$$(\\nabla_\\mathcal{G})_{(i,j)} \\ f = \\sqrt{W_{ij}} (f_j - f_i)$$\n",
    "\n",
    "We may easily obtain it by calling `graphs.compute_differential_operator()` command as follows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.compute_differential_operator()\n",
    "gradient = G.D @ signal\n",
    "assert gradient.size == G.Ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compute the divergence of an edge signal, which is again a signal on the nodes.\n",
    "\n",
    "$$(\\operatorname{div}_\\mathcal{G} x)_i = \\sum_{j \\sim i} \\sqrt{W_{ji}} x_{(j,i)} - \\sum_{i \\sim j} \\sqrt{W_{ij}} x_{(i,j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divergence = G.D.T @ gradient\n",
    "assert divergence.size == G.N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplacian operator is the divergence of the gradient by definition. Let us check if we obtain the same output signal by taking the Laplacian of the signal we generated;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(17, 6))\n",
    "G.plot_signal(divergence, ax=axes[0])\n",
    "G.plot_signal(G.L @ signal, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Smoothness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothness of a signal can be computed by the quadratic form\n",
    "\n",
    "$$ f^\\intercal L f = \\| \\nabla_\\mathcal{G} f \\|_2^2 = \\sum_{i \\sim j} W_{ij} (f_j - f_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.T @ G.L @ signal / np.linalg.norm(signal)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare it with the partitioning function:\n",
    "$$ x[i] =\n",
    "\\begin{cases}\n",
    "    -1 &\\text{if } i \\in S_1, \\\\\n",
    "    0  &\\text{if } i \\in S_2, \\\\\n",
    "    1  &\\text{if } i \\in S_3,\n",
    "\\end{cases}\n",
    "$$\n",
    "where $S_i$ is the set of nodes in partition $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = G.info['node_com'] - 1\n",
    "G.plot_signal(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.T @ G.L @ x / np.linalg.norm(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That function is certainly smoother!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Graph Fourier transform\n",
    "\n",
    "We can analyze the spectral content of a graph signal through the graph Fourier transform. It indicates how the signal variates on the connectivity pattern of the graph, i.e.; low-pass, band-pass, or high-pass.\n",
    "\n",
    "We simply call `graphs.compute_fourier_basis()` command to compute the Fourier basis of a graph;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.compute_fourier_basis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also asses the smoothness of the signals we have generated above by plotting their Fourier transform on the graph spectrum;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(17, 10))\n",
    "\n",
    "signal_hat = G.gft(signal)  # Fourier transform of the random signal\n",
    "x_hat = G.gft(x)  # Fourier transform of the partition signal\n",
    "\n",
    "G.plot_signal(signal, ax=axes[0, 0])\n",
    "G.plot_signal(x, ax=axes[1, 0])\n",
    "\n",
    "axes[0, 1].plot(G.e, np.abs(signal_hat), '.-')\n",
    "axes[1, 1].plot(G.e, np.abs(x_hat), '.-')\n",
    "\n",
    "\n",
    "axes[0, 0].set_title('random signal in the vertex domain')\n",
    "axes[1, 0].set_title('partition signal in the vertex domain')\n",
    "axes[0, 1].set_title('random signal in the spectral domain')\n",
    "axes[1, 1].set_title('partition signal in the spectral domain')\n",
    "axes[1, 0].set_xlabel(\"graph spectrum\")\n",
    "axes[1, 1].set_xlabel(\"graph spectrum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph Signal Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph signal $\\mathbf{x}$ is filtered as\n",
    "$$\\mathbf{y} = \\mathbf{U} \\hat{g}(\\mathbf{\\Lambda}) \\mathbf{U}^\\intercal \\, \\mathbf{x} = \\hat{g}(\\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^\\intercal) \\, \\mathbf{x} = \\hat{g}(\\mathbf{L}) \\, \\mathbf{x},$$\n",
    "\n",
    "where $\\hat{g}(\\cdot)$ is the filter kernel defined in the specral domain as a function of the eigenvalues (\"frequencies\"). We can further separate the filtering operation into the following steps:\n",
    "\n",
    "1. We obtain the spectral domain representation of the signal by GFT: \n",
    "$\\hat{\\mathbf{x}} = \\mathbf{U}^\\intercal \\, \\mathbf{x}$.\n",
    "2. We filter the signal on the spectral domain by multiplicating each Fourrier coeeficient with the corresponding filter coeffcient, i.e.; elementwise multiplication: $\\hat{\\mathbf{y}} = \\hat{g} \\odot \\hat{\\mathbf{x}} = \\hat{g}(\\mathbf{\\Lambda})\\hat{\\mathbf{x}} $\n",
    "3. We return back to the vertex domain by taking the IGFT of the filtered signal: $\\mathbf{y} = \\mathbf{U}\\hat{\\mathbf{y}}$\n",
    "\n",
    "\n",
    "\n",
    "You can also consider the filtering operation as a matrix vector multiplication between the filtering operator, $\\hat{g}(\\mathbf{L})$, and the graph signal $\\mathbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`filters`](https://pygsp.readthedocs.io/en/stable/reference/filters.html) module on PyGSP includes the implementation of commonly used graph filters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple Filtering: heat diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us accomplish a simple filtering operation using the heat kernel $h(\\lambda)$ is defined as:\n",
    "$$h_\\tau(\\lambda)=\\exp^{-\\tau\\lambda}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may visualize the heat diffussion on a manifold graph by generating a filter bank of heat kernels with different $\\tau$ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graphs.Bunny()  # Stanford bunny manifold graph.\n",
    "G.compute_fourier_basis()  # Construct the Fourrier basis of the graph.\n",
    "taus = [5, 20, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a signal as a Kronecker delta located on one vertex, e.g. the vertex 20. That signal is our heat source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.zeros(G.N)\n",
    "DELTA = 20\n",
    "s[DELTA] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `filters.filter()` function to realize the filtering operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 10))\n",
    "\n",
    "for i in range(len(taus)):\n",
    "    g = filters.Heat(G, taus[i])  # kernel constructed specifically on the spectrum of graph G\n",
    "    s_out = g.filter(s, method='chebyshev')\n",
    "    ax = fig.add_subplot(2, len(taus), i+1, projection='3d')\n",
    "    G.plot_signal(s_out, colorbar=False, ax=ax)\n",
    "    title = r'Heat diffusion, $\\tau={}$'.format(taus[i])\n",
    "    _ = ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "    ax = fig.add_subplot(2, len(taus), i+4)\n",
    "    g.plot(ax=ax)\n",
    "#fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Localization**: We can also localize a kernel on a particular part of our manifold graph using `filtes.localize()` command as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "\n",
    "g = filters.Heat(G, tau = 100)  # The heat kernel to be localized\n",
    "DELTA = [20, 30, 300]  # node to localize the heat kernel\n",
    "\n",
    "for i in range(len(DELTA)):\n",
    "    s_localized = g.localize(DELTA[i])\n",
    "    ax = fig.add_subplot(1, len(DELTA), i+1, projection='3d')\n",
    "    G.plot_signal(s_localized, colorbar=False, ax=ax)\n",
    "    title = r'Heat kernel localized at node-{}'.format(DELTA[i])\n",
    "    _ = ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "#fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Example of denoising using Tikhonov regularization\n",
    "\n",
    "Let's define a low-pass filter\n",
    "$$ g(\\lambda) = \\frac{1}{1+\\tau\\lambda} $$\n",
    "\n",
    "Given a noisy version of a smooth signal $x_\\text{noisy}$, one can denoise it with the low-pass filter $g$:\n",
    "$$ x_\\text{denoised} = \\mathbf{U}g(\\mathbf{\\Lambda})\\mathbf{U}^\\top x_{\\text{noisy}}, $$\n",
    "which corresponds to the filtering operator: $g(L)  = (I + \\tau L)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a custom filter using by defining a kernel function and calling the command `filters.Filter()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1\n",
    "def g(x):\n",
    "    return 1. / (1. + tau * x)\n",
    "g = filters.Filter(G, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = s_localized\n",
    "# add some noise\n",
    "x_noisy = x +  np.sqrt(np.var(x)/3) * np.random.randn(G.N)\n",
    "\n",
    "# the denoised signal:\n",
    "x_denoised = g.filter(x_noisy, method='exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "G.plot_signal(x_noisy, colorbar=False, ax=ax)\n",
    "ax.set_title('Noisy Signal')\n",
    "ax.set_axis_off()\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "G.plot_signal(x_denoised, colorbar=False, ax=ax)\n",
    "ax.set_title('Denoised Signal')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Wavelet Decomposition of a Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now create a filter bank using one of the predefined filters, such as `filters.MexicanHat` to design a set of [Mexican hat wavelets](https://en.wikipedia.org/wiki/Mexican_hat_wavelet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = filters.MexicanHat(G, Nf=4)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(17, 5))\n",
    "g.plot(ax=ax)\n",
    "plt.title('Filter bank of mexican hat wavelets');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose a graph signal to the wavelet componenets by calling the `analyze()` method of a filter object. Then, we may try to recover our signal by using only those wavelet coefficients by calling `synthesize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = x  # original signal\n",
    "s2 = g.analyze(s1)  # wavelet coefficients\n",
    "s3 = g.synthesize(s2)  # reconstructed signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the reconstruction error given the wavelet representation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "G.plot_signal(s1, colorbar=False, ax=ax)\n",
    "ax.set_title('Original Signal')\n",
    "ax.set_axis_off()\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "G.plot_signal(s3, colorbar=False, ax=ax)\n",
    "ax.set_title('Reconstructed Signal')\n",
    "ax.set_axis_off()\n",
    "print('Reconstruction error: {:.5f}'.format(np.linalg.norm(s1 - s3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try to do the wavelet decomposition using `filters.Itersine` with `Nf=4` filters and then synthesize the signal back. How do you explain the difference in reconstruction error between `Itersine` and `MexicanHat` representations? Hint: You may check the frequency response of `Itersine`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
